# 27 â€“ Evaluationsrahmen

Dieses Kapitel beschreibt den **Evaluationsrahmen**, mit dem BitGridAI im Rahmen einer empirischen Studie untersucht wird.
Ziel ist es, die Wirkung eines **Explainability-Layers** im Vergleich zu einer **Baseline-UI** systematisch zu bewerten.

Der Fokus liegt auf dem **VerstÃ¤ndnis der Entscheidungslogik**, dem **Vertrauen der Nutzer:innen**, der **wahrgenommenen Kontrolle**, der **kognitiven Belastung** sowie auf **energiebezogenen Effekten**.

&nbsp;

## Ãœberblick

Die Evaluation ist als **Between-Subjects-Studie** angelegt, in der zwei Systemvarianten verglichen werden:

* **Baseline-Variante**: Anzeige von ZustÃ¤nden und Aktionen ohne erklÃ¤renden Layer
* **Explainability-Variante**: Anzeige von ZustÃ¤nden inklusive erklÃ¤render BegrÃ¼ndungen gemÃ¤ÃŸ Kapitel 24

Die Studie kombiniert **technische Messungen** mit **nutzerzentrierten Erhebungsmethoden**.

&nbsp;

## Evaluationsziele

Die Evaluation verfolgt fÃ¼nf zentrale Ziele:

1. **ErklÃ¤rbarkeit messen**
   Verstehen Nutzer:innen die GrÃ¼nde fÃ¼r Start-, Stop- und NOOP-Entscheidungen?
2. **Vertrauen und Kontrolle bewerten**
   FÃ¼hlen sich Nutzer:innen informiert und handlungsfÃ¤hig?
3. **Kognitive Belastung erfassen**
   ErhÃ¶hen erklÃ¤rende Informationen die mentale Belastung?
4. **Energiebezogene Effekte analysieren**
   Unterscheiden sich Energieverbrauch und Schaltverhalten zwischen den Varianten?
5. **Transparenz validieren**
   Sind UI-BegrÃ¼ndungen und Systemlogs konsistent?

&nbsp;

## Studiendesign

* **Design:** Between-Subjects (Baseline vs. Explainability)
* **Stichprobe:** N = 10, heterogener technischer Hintergrund
* **Dauer:** 10 Tage
* **TÃ¤glicher Aufwand:** ca. 10â€“15 Minuten
* **Methodischer Ansatz:** Daily Diary Method kombiniert mit Abschlussinterviews

### Aufgaben

Die Teilnehmenden bearbeiten wiederkehrende Aufgaben, u. a.:

* PrÃ¼fung von PV- und SpeicherzustÃ¤nden
* Einordnung von Start- und Stop-Entscheidungen
* Bewertung von NOOP-Situationen
* Test manueller Overrides

### Setting

* Smart-Home-Laborumgebung
* Simulierte PV- und Batterieprofile
* Reale, steuerbare Lasten

&nbsp;

## Methodik

| Ebene                    | Methode                                     | Ziel                                            |
| ------------------------ | ------------------------------------------- | ----------------------------------------------- |
| **Systemebene**          | Logging, Energiemessung, Variantenvergleich | Analyse von Schaltverhalten und Energieeffekten |
| **Nutzerebene**          | Daily Diary, Leitfaden-Interviews           | VerstÃ¤ndnis, Vertrauen, mentale Modelle         |
| **Interaktionsebene**    | Task-basierte Tests, Override-Szenarien     | Klarheit, Task-Zeit, Fehlannahmen               |
| **Qualitative Analyse**  | Inhaltsanalyse                              | Muster in Wahrnehmung und Vertrauen             |
| **Quantitative Analyse** | Standardisierte Skalen und Metriken         | Vergleichbarkeit der Bedingungen                |

&nbsp;

## Evaluationsumgebung

### Hardware

* x86 Mini-PC mit lokalem System (z. B. UmbrelOS)
* Tablet als Smart-Home-Dashboard
* Steuerbare ASIC-Lasten (z. B. Bitaxe Gamma, NerdQaxe++)
* Messsteckdosen (z. B. Shelly Plug S Gen3)

### Software & KI

* Lokales Dashboard in zwei UI-Varianten
* Lokales LLM via Ollama
* Quantisierte Modelle (z. B. Phi-3 Mini, Mistral 7B)

### Datenbasis

* Simulierte PV- und Batterieprofile
* Reale Telemetriedaten der Lasten
* Strukturierte JSON-Logs (Entscheidungen, GrÃ¼nde, Overrides)

&nbsp;

## Erhebungsinstrumente

* **Daily Diary**: kurze tÃ¤gliche EintrÃ¤ge Ã¼ber Wahrnehmung und VerstÃ¤ndnis
* **Leitfaden-Interviews**: Vertiefung von Vertrauen und mentalen Modellen
* **FragebÃ¶gen**:

  * SUS (Usability)
  * NASA-TLX (kognitive Belastung)
* **Systemlogs**: Entscheidungen, RegelzustÃ¤nde, EnergieflÃ¼sse

&nbsp;

## Bewertungsmetriken

| Kategorie            | Metrik                            | Beschreibung                                 |
| -------------------- | --------------------------------- | -------------------------------------------- |
| **Explainability**   | VerstÃ¤ndnisrate (%)               | Anteil korrekt erklÃ¤rter Entscheidungen      |
| **Trust & Control**  | Vertrauen (Likert), Override-Rate | Subjektives Vertrauen und Eingriffsverhalten |
| **Cognitive Load**   | NASA-TLX Score                    | Mentale Belastung pro Sitzung                |
| **Usability**        | SUS, Task-Zeit                    | Subjektive Usability und objektive Dauer     |
| **Energy Behaviour** | Schaltungen/Tag, Laufzeiten       | Systemruhe und Steuerungsverhalten           |
| **Transparency**     | Log-Konsistenz                    | Ãœbereinstimmung Log â†” UI                     |

&nbsp;

## Auswertung & Dokumentation

* Vergleich der beiden UI-Varianten Ã¼ber alle Metriken
* Triangulation aus Logs, Diaries, Interviews und FragebÃ¶gen
* Dokumentation der Ergebnisse in internen Dashboards oder Notebooks

Der Schwerpunkt liegt auf der **ErklÃ¤rqualitÃ¤t** und deren Einfluss auf Vertrauen, VerstÃ¤ndnis und Nutzung.

&nbsp;

## Zusammenfassung

Der Evaluationsrahmen verbindet **technische Systemdaten** mit **nutzerzentrierter Evaluation**, um die Wirkung eines erklÃ¤renden KI-Layers empirisch zu untersuchen.

Er liefert damit eine fundierte Grundlage fÃ¼r die Bewertung transparenter, lokal ausgefÃ¼hrter Energiemanagementsysteme.



---

> **NÃ¤chster Schritt:** Der Evaluationsrahmen steht.
> Im nÃ¤chsten Kapitel folgen Reflexion & Transfer.
>
> ğŸ‘‰ Weiter zu **[28 - Reflexion & Transfer](../28_reflection_and_transfer/README.md)**
>
> ğŸ”™ ZurÃ¼ck zu **[2 - Forschung](../README.md)**
>
> ğŸ  ZurÃ¼ck zur **[HauptÃ¼bersicht](../../README.md)**
