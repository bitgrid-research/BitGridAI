# 10.2.1 - Transparenz & ErklÃ¤rbarkeit (Explainability)

Keine Entscheidung ohne Warum.

BitGridAI trifft Entscheidungen, die reale Auswirkungen haben:  
EnergieflÃ¼sse, HardwarezustÃ¤nde, Kosten.  
Damit Nutzer dem System vertrauen kÃ¶nnen, mÃ¼ssen diese Entscheidungen **jederzeit nachvollziehbar, erklÃ¤rbar und Ã¼berprÃ¼fbar** sein.

Dieses QualitÃ¤tsszenario beschreibt, wie BitGridAI Transparenz sicherstellt â€“ nicht als Zusatzfunktion, sondern als **architektonisches Grundprinzip**.

Grundsatz:
> **Was nicht erklÃ¤rt werden kann, darf nicht automatisch entscheiden.**

(Platzhalter fÃ¼r ein Bild: Ein Pixel-Art-Hamster mit Lupe und Klemmbrett steht vor einem Entscheidungsdiagramm. Sprechblase: â€Darum habe ich das so gemacht.â€œ)

&nbsp;

## QualitÃ¤tsziel

**Jede automatische Entscheidung ist fÃ¼r den Nutzer verstÃ¤ndlich erklÃ¤rbar**,  
inklusive AuslÃ¶ser, Regelbasis und relevanter Grenzwerte.

&nbsp;

## Kontext

- Regelbasierte Entscheidungen (R1â€“R5) im Block-Takt (Kap. 06)
- Entscheidungen werden als `DecisionEvent` persistiert
- Explainability erfolgt **on-device**, ohne Cloud-AbhÃ¤ngigkeit
- UI zeigt Zustand, Historie und Prognose an (Kap. 08.3)

&nbsp;

## Szenario E-1: Nutzer fragt â€Warum lÃ¤uft der Miner gerade?â€œ

**Stimulus:**  
Der Nutzer stellt im UI eine Explain-Anfrage.

**Quelle:**  
BenutzeroberflÃ¤che / Explain-UI

**Betriebszustand:**  
Mining aktiv (halb- oder vollautomatischer Modus)

**Erwartete Systemreaktion:**
- Identifikation des aktuell wirksamen `DecisionEvent`
- Erzeugung einer Explain-Session mit:
  - angewendeter Regel (z.B. R1, R4)
  - auslÃ¶senden Messwerten
  - relevanten Schwellenwerten
- Ausgabe einer verstÃ¤ndlichen, menschenlesbaren ErklÃ¤rung

**Akzeptanzkriterien:**
- ErklÃ¤rung innerhalb von **â‰¤ 2 Sekunden**
- Bezug auf konkrete Messwerte und Regeln
- Keine generischen oder ausweichenden Antworten

&nbsp;

## Szenario E-2: Entscheidung wird automatisch geÃ¤ndert

**Stimulus:**  
System stoppt Mining aufgrund verÃ¤nderter Bedingungen (z.B. sinkender PV-Ãœberschuss).

**Quelle:**  
Rule Engine

**Betriebszustand:**  
Automatikbetrieb

**Erwartete Systemreaktion:**
- Neues `DecisionEvent` wird erzeugt
- Explain-Daten (Reason, Trigger, Parameter) werden persistiert
- UI aktualisiert Timeline und zeigt Ursache der Ã„nderung

**Akzeptanzkriterien:**
- Jede ZustandsÃ¤nderung ist erklÃ¤rbar
- Historische Entscheidungen bleiben abrufbar
- Ursache ist eindeutig identifizierbar

&nbsp;

## Szenario E-3: Vorschau auf die nÃ¤chste Entscheidung

**Stimulus:**  
Der Nutzer betrachtet die â€Next-Block-Previewâ€œ im UI.

**Quelle:**  
UI / Explain-Agent

**Betriebszustand:**  
Beliebig

**Erwartete Systemreaktion:**
- Simulation der nÃ¤chsten Regelbewertung
- Anzeige der **erwarteten** Aktion inkl. BegrÃ¼ndung
- Kennzeichnung als Prognose, nicht als Entscheidung

**Akzeptanzkriterien:**
- Klare Trennung zwischen Ist-Entscheidung und Vorschau
- Vorschau basiert auf aktuellen Daten und aktiver Konfiguration
- Keine verdeckten Automatismen

&nbsp;

## Messbare QualitÃ¤tsmerkmale

| Merkmal | Ziel |
|-------|------|
| ErklÃ¤rungsabdeckung | 100 % aller DecisionEvents |
| Explain-Latenz | â‰¤ 2 s |
| Cloud-AbhÃ¤ngigkeit | 0 |
| Verweis auf Regel & Trigger | immer vorhanden |

&nbsp;

## Bezug zur Architektur

- **Explain-Agent & UI:** Kap. 08.3 / 08.4  
- **DecisionEvent-Modell:** Kap. 08.1  
- **Replay & Audit:** Kap. 08.8  
- **ADRs:** Explainability on-device (Kap. 09)

&nbsp;

## Zusammenfassung

Explainability ist kein UI-Gimmick, sondern **Voraussetzung fÃ¼r Autonomie**.

BitGridAI:
- trifft nur erklÃ¤rbare Entscheidungen,
- macht GrÃ¼nde sichtbar statt sie zu verstecken,
- und ermÃ¶glicht Vertrauen durch Transparenz.

---

> **NÃ¤chster Schritt:**  
> Transparenz schafft Vertrauen â€“ echte Kontrolle entsteht jedoch erst,
> wenn der Nutzer entscheiden darf, **wer die Kontrolle hat**.
>
> ğŸ‘‰ Weiter zu **[10.2.2 - Autonomie & Privacy](./1022_autonomy_and_privacy.md)**
>
> ğŸ”™ ZurÃ¼ck zur **[KapitelÃ¼bersicht](./README.md)**
