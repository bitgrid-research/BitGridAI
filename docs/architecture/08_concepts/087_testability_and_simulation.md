# 08.7 Testbarkeit & Simulation

H√§rtetest ohne Hardware.

Da wir in der realen Welt agieren, k√∂nnen wir nicht st√§ndig mit echten Wechselrichtern und hei√üen Minern testen. Die Kosten und die Zeit f√ºr physische Tests w√§ren immens.

Daher ist es ein fundamentales architektonisches Ziel, dass das **gesamte System ohne physische Hardware lauff√§hig und testbar** ist. Die strikte Anwendung der **Hexagonalen Architektur** (Trennung von Core und Adaptern) und das Prinzip der **deterministischen Regeln (R1‚ÄìR5)** sind hier unsere wichtigsten Werkzeuge.

*(Platzhalter f√ºr ein Bild: Ein Hamster sitzt vor einem Monitor, auf dem ein kompliziertes Sequenzdiagramm zu sehen ist, und f√ºhrt mit einem kleinen Roboter-Hamster im Sandkasten einen Test durch.)*
![Hamster f√ºhrt Simulation durch](../../media/pixel_art_hamster_simulation.png)

## 1. Test-Arten & Determinismus

Dank der strikten Trennung und der regelbasierten Logik k√∂nnen wir verschiedene Test-Level effektiv anwenden:

| Test-Art | Gegenstand | Ziel |
| :--- | :--- | :--- |
| **Unit-Tests** | Regeln R1‚ÄìR5, Schwellen, Hysterese, Priorit√§ten. | Pr√ºfen der Kernlogik: Muss `R3 > R2 > R5 > R1/R4` gelten? Funktioniert der Schwellwert f√ºr 1.5 kW? |
| **Integrationstests** | MQTT Topics, REST Endpunkte, Adapter-Logik. | Pr√ºfen der Schnittstellen: F√ºhrt `POST /override` zu einem `DecisionEvent` im UI? |
| **Replay-Tests** | Gespeicherte Parquet/SQLite-Logs. | **Reproduzierbarkeit.** Abspielen historischer Daten in Echtzeit oder beschleunigt (`--speed 10x`) und Vergleichen der Entscheidungen. |
| **A/B-Tuning** | Deadband-L√§ngen, Forecast-Margins. | Vergleichen neuer **Policies** (z.B. neue Deadband-L√§nge) gegen eine Baseline (Historie) zur KPI-Optimierung. |

## 2. Der Replay-Runner (Audit & Forschung) üéì

Dies ist das m√§chtigste Werkzeug, um die Determinismus-Anforderung zu pr√ºfen.

* **Werkzeug:** `bitgrid-replay` CLI.
* **Funktion:** Nimmt ein Bundle aus historischen Parquet-Logs (`--state data/parquet/*.parq`) und einer Konfigurationsdatei (`--config config/rules.yaml`) entgegen und f√ºttert den `Rule Engine Core` damit.
* **Audit:** Wenn ein Replay mit alten Logs und alter Config **nicht** die exakt gleichen `DecisionEvents` ergibt, ist ein Fehler im Code vorhanden.

## 3. Werkzeuge und Fault-Injection

Um das System unter Stress zu setzen, nutzen wir gezielte Fehler-Injektionen:

* **Fault-Injection:** Simulierte Ausf√§lle, die zu einem Notfall (R3) f√ºhren sollen.
    * *Beispiele:* `Sensor-Stale` (Datenfluss stoppt), `Broker-Down` (MQTT bricht zusammen), simulierte `Heat-Events`.
    * *Zweck:* Verifizieren, dass der **Circuit-Breaker** und die Not-Stopp-Kette korrekt greifen (siehe Runbooks in 08.5).
* **Feature-Flags:** Policies k√∂nnen zur Laufzeit √ºber Feature-Flags im UI oder in der Config umgeschaltet werden (z.B. `r4_enabled=false`), um die Wirkung einzelner Regeln zu isolieren.

## 4. Erfolgskriterien (KPIs)

Die Wirkung der Tests und des Tunings messen wir √ºber Kennzahlen:

| KPI | Zielwert | Zweck |
| :--- | :--- | :--- |
| **Decision Latency** | < 300 ms | Zeit zwischen Block-Tick und finaler Entscheidung (Performance des Cores). |
| **Explanation Latency** | < 2 s | Zeit zwischen `DecisionEvent` und der fertigen Text-Erkl√§rung im UI (UX). |
| **Thermal Incidents** | 0 | Nach Updates oder neuen Konfigurationen darf R3 nicht ausgel√∂st werden. |
| **Flapping Rate (‚Üì)** | R√ºckgang gg√º. Baseline | Misst die Effizienz von R5. Hohe Rate $\rightarrow$ schlechte Stabilit√§t. |
| **Grid Import (‚Üì)** | Minimierung | √ñkonomischer Erfolg. Wie gut nutzt das System den PV-√úberschuss? |
| **Explanation Coverage** | 100 % | Jedes `DecisionEvent` muss einen g√ºltigen `Reason` und `Trigger` enthalten (Auditierbarkeit). |

---
> **N√§chster Schritt:** Wir k√∂nnen das System entwickeln, testen und simulieren. Nun fehlt nur noch der Weg in die Produktion.
>
> üëâ Weiter zu **[08.8 Build- & Release-Management](./088_build_managment.md)**
>
> üîô Zur√ºck zur **[Kapitel√ºbersicht](./README.md)**
