# 8.8 - Testbarkeit, Simulation & Replays

HÃ¤rtetest ohne Hardware.

BitGridAI agiert in der realen Welt: mit Wechselrichtern, Batterien und potenziell heiÃŸen Minern.  
Physische Tests sind teuer, langsam und riskant â€“ und fÃ¼r viele Szenarien schlicht nicht praktikabel.

Daher ist es ein fundamentales architektonisches Ziel, dass **das gesamte System ohne physische Hardware lauffÃ¤hig, testbar und Ã¼berprÃ¼fbar ist**.  
MÃ¶glich wird dies durch:
- die strikte Trennung von Core und Adaptern (hexagonale Architektur),
- deterministische Regeln (R1â€“R5),
- immutable ZustÃ¤nde und blockbasierten Takt.

*(Platzhalter fÃ¼r ein Bild: Ein Pixel-Art-Hamster testet ein komplexes System in einer Sandkasten-Simulation, wÃ¤hrend echte Hardware sicher im Hintergrund bleibt.)*
![Hamster fÃ¼hrt Simulation durch](../../media/pixel_art_hamster_simulation.png)

&nbsp;

## Ziel: Deterministische Entscheidungen

Grundprinzip:
> **Eine Entscheidung ist nur dann vertrauenswÃ¼rdig, wenn sie reproduzierbar ist.**

Testbarkeit in BitGridAI bedeutet:
- gleiche Eingaben â†’ gleiche Entscheidungen,
- klare Trennung von Logik und Umwelt,
- Ã¼berprÃ¼fbares Verhalten auch lange nach dem Live-Betrieb.

Determinismus ist kein Test-Feature, sondern ein **Architekturversprechen**.

&nbsp;

## Testarten & Ebenen

Dank der klaren Trennung von Logik und I/O kÃ¶nnen verschiedene Testebenen systematisch eingesetzt werden:

| Testart | Gegenstand | Ziel |
|------|-----------|------|
| **Unit-Tests** | Regeln R1â€“R5, Schwellen, PrioritÃ¤ten, Hysterese | Korrektheit der Kernlogik (z.B. Vorrang R3 vor R2/R1) |
| **Integrationstests** | Schnittstellen (MQTT, REST), Adapter-Anbindung | Korrekte Systemreaktionen auf externe Signale |
| **Replay-Tests** | Historische Zustands- & Entscheidungsdaten | **Reproduzierbarkeit** vergangener Entscheidungen |
| **Policy- & A/B-Tuning** | Deadbands, Forecast-Margen, Strategien | Vergleich neuer Regelparameter gegen Baselines |

Diese Ebenen bauen aufeinander auf und adressieren jeweils unterschiedliche Risiken.

&nbsp;

## Simulation

Simulation ersetzt reale Umwelt durch kontrollierte Datenquellen.

Typische Simulationsinhalte:
- PV-Erzeugungsprofile
- StrompreisverlÃ¤ufe
- Batterie- und Lastmodelle
- Adapter- und Sensor-AusfÃ¤lle

Simulationen sind:
- reproduzierbar,
- zeitlich steuerbar,
- frei kombinierbar.

Sie ermÃ¶glichen Tests von:
- Grenz- und ExtremfÃ¤llen,
- seltenen FehlerzustÃ¤nden,
- neuen Regelparametern â€“ ohne reale Hardware zu gefÃ¤hrden.

&nbsp;

## Replays (Audit & Forschung)

Replays sind ein zentrales Werkzeug fÃ¼r QualitÃ¤tssicherung und Audit.

Ein Replay:
- nutzt historische Zustandsdaten,
- spielt diese deterministisch erneut ab,
- erzeugt Entscheidungen und Events neu.

Replays dienen:
- Regressionstests nach Code- oder Config-Ã„nderungen,
- Analyse vergangener Entscheidungen,
- Vergleich alternativer Strategien.

Replays sind strikt **read-only** und beeinflussen niemals das Live-System.

&nbsp;

## Fault Injection & Robustheit

Zur gezielten ÃœberprÃ¼fung von Fail-safe- und Degradationslogik werden Fehler bewusst simuliert, z.B.:

- ausbleibende Sensordaten,
- Adapter- oder Broker-AusfÃ¤lle,
- Ãœbertemperatur-Szenarien.

Ziel ist nicht das â€Durchhalten um jeden Preisâ€œ, sondern:
- korrektes Umschalten in Safe- oder Stop-ZustÃ¤nde,
- klare Events und ErklÃ¤rungen,
- kontrollierte Recovery-Pfade.

Diese Tests validieren direkt die Prinzipien aus Kapitel 8.6.

&nbsp;

## Bewertung & Erfolgskriterien (KPIs)

Die Wirkung von Tests, Simulationen und Regelanpassungen wird Ã¼ber messbare Kennzahlen bewertet:

| KPI | Ziel | Zweck |
|----|------|------|
| **Decision Latency** | niedrig | Performance des Cores |
| **Explanation Latency** | niedrig | UX-QualitÃ¤t |
| **Thermal Incidents** | 0 | Safety-Garantie |
| **Flapping Rate** | sinkend | Wirksamkeit von R5 |
| **Grid Import** | minimiert | Ã–konomischer Erfolg |
| **Explainability Coverage** | 100 % | Auditierbarkeit |

KPIs verbinden technische QualitÃ¤t mit realer Systemwirkung.

&nbsp;

## Testbarkeit & Betrieb

Testbarkeit ist Voraussetzung fÃ¼r sicheren Betrieb:

- Updates werden erst nach erfolgreichen Replays freigegeben,
- neue Konfigurationen kÃ¶nnen vorab simuliert werden,
- Rollbacks basieren auf bekannten, geprÃ¼ften ZustÃ¤nden.

Damit bildet Testbarkeit die BrÃ¼cke zwischen Entwicklung und Betrieb.

&nbsp;

## Abgrenzungen

Nicht Bestandteil dieses Kapitels sind:
- konkrete Testframeworks oder CLI-Tools,
- CI/CD-Implementierungen,
- detaillierte Testskripte.

Diese gehÃ¶ren in Entwickler- oder Betriebsdokumentation.

&nbsp;

## Zusammenfassung

Testbarkeit, Simulation und Replays stellen sicher, dass BitGridAI:
- zuverlÃ¤ssig weiterentwickelt werden kann,
- Entscheidungen Ã¼berprÃ¼fbar bleiben,
- Vertrauen durch Wiederholbarkeit entsteht.

BitGridAI wird nicht nur betrieben â€“  
es wird **bewiesen**.

---

> **NÃ¤chster Schritt:**  
> Wenn QualitÃ¤t geprÃ¼ft ist, stellt sich die Frage nach kontrollierter Auslieferung.  
> Im nÃ¤chsten Kapitel betrachten wir die **Build-, Update- & Release-Prinzipien**.
>
> ğŸ‘‰ Weiter zu **[8.9 Build-, Update- & Release-Prinzipien](./089_build_and_release.md)**
>
> ğŸ”™ ZurÃ¼ck zur **[KapitelÃ¼bersicht](./README.md)**
